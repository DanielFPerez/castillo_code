{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ebe521-c597-4800-9234-280ced28aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    GenerationConfig,  \n",
    "    DynamicCache, StaticCache)\n",
    "import accelerate\n",
    "import torch\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9c4140-dc53-4d8d-ac56-52e687fe6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c51c5a-6769-4e60-844d-89cec0352478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3875bd-3b33-465d-84d5-69f97230b497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daniel/.cache/huggingface/hub/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd86369710c428cbd1e67ab1480903b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "access_tk = os.getenv(\"HF_API_TOKEN\")\n",
    "device_str = \"cuda:0\"\n",
    "#device_str = \"auto\"\n",
    "\n",
    "cache_dir = os.getenv(\"HF_HOME\", os.path.expanduser(\"~/.cache/huggingface/hub/\"))\n",
    "print(cache_dir)\n",
    "\n",
    "#model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\" #\"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "#model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "padding_side='left'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=access_tk, device_map=device_str, \n",
    "                                             torch_dtype=torch.float16, cache_dir=cache_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_tk, \n",
    "                                          torch_dtype=torch.float16, padding_side=padding_side, \n",
    "                                          cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "764b4b02-a71c-4fde-8df5-0404545f7cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d95431-f8fc-4e8c-a8c5-3093123c7f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vocab_size', 'max_position_embeddings', 'hidden_size', 'intermediate_size', 'num_hidden_layers', 'num_attention_heads', 'num_key_value_heads', 'hidden_act', 'initializer_range', 'rms_norm_eps', 'pretraining_tp', 'use_cache', 'rope_theta', 'rope_scaling', 'attention_bias', 'attention_dropout', 'mlp_bias', 'head_dim', 'return_dict', 'output_hidden_states', 'output_attentions', 'torchscript', 'torch_dtype', 'use_bfloat16', 'tf_legacy_loss', 'pruned_heads', 'tie_word_embeddings', 'chunk_size_feed_forward', 'is_encoder_decoder', 'is_decoder', 'cross_attention_hidden_size', 'add_cross_attention', 'tie_encoder_decoder', 'max_length', 'min_length', 'do_sample', 'early_stopping', 'num_beams', 'num_beam_groups', 'diversity_penalty', 'temperature', 'top_k', 'top_p', 'typical_p', 'repetition_penalty', 'length_penalty', 'no_repeat_ngram_size', 'encoder_no_repeat_ngram_size', 'bad_words_ids', 'num_return_sequences', 'output_scores', 'return_dict_in_generate', 'forced_bos_token_id', 'forced_eos_token_id', 'remove_invalid_values', 'exponential_decay_length_penalty', 'suppress_tokens', 'begin_suppress_tokens', 'architectures', 'finetuning_task', 'id2label', 'label2id', 'tokenizer_class', 'prefix', 'bos_token_id', 'pad_token_id', 'eos_token_id', 'sep_token_id', 'decoder_start_token_id', 'task_specific_params', 'problem_type', '_name_or_path', '_commit_hash', '_attn_implementation_internal', '_attn_implementation_autoset', 'transformers_version', 'model_type'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84e23d40-2959-4e99-818b-8bcbbd80b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_new_tokens = model.config.max_position_embeddings - 2048\n",
    "max_new_tokens = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda9bd7f-64c8-4002-a85b-47a07a691e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_cache = DynamicCache()\n",
    "# prompt_cache.get_max_cache_shape()\n",
    "#prompt_cache = StaticCache(config=model.config, max_batch_size=10, max_cache_len=max_new_tokens, \n",
    "#                           device=\"cuda:0\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806a18eb-ac71-4be5-8d6e-4b8353d3ea59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b323e0a2-2806-45ac-963f-f8ab3c05f3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.config.name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a01c253e-b7f6-4361-b25d-7bdafa62e2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.is_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29146328-55f7-44df-a1fe-0ae66597c376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3212.749824"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71150a87-4577-4a6a-9a8d-234795143ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33132521-9b03-45e7-a594-46614cae4af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d0bf873-98d8-4096-aa5e-5d5110db8281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cc8d73b-c9c0-4ca8-b09a-04f11f261cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17.953193984, 25.425608704]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[elem/1e9 for elem in torch.cuda.mem_get_info()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b726841f-7325-4d17-ba49-b3635fe7c044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17.953193984, 25.425608704]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[elem/1e9 for elem in torch.cuda.mem_get_info()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d1ce6-b61f-429d-b4b5-d0213ac33371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11fda5fa-78db-49ba-b4d2-8dad0c172b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd4aa805-2253-48fc-b0ad-55a4af24d0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "313fc6df-b899-4482-be9b-2fc8894f8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0422d26a-f206-4afe-ac72-34ae007ef932",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12d896e2-6ee4-41ea-92fa-cbcd7f92feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "add_generation_prompt = True\n",
    "padding = True\n",
    "return_tensors = \"pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "632f65a2-3577-4b34-bd01-888a7f5d4b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|eot_id|>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60363e84-173c-4738-b6e5-9bcd58544b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b9f7523-2f8e-4049-8057-23e0b88523c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_name, token=access_tk)\n",
    "\n",
    "generation_config.output_hidden_states = True\n",
    "generation_config.output_logits = True\n",
    "generation_config.return_dict_in_generate = True\n",
    "generation_config.max_new_tokens = max_new_tokens\n",
    "generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "generation_config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45d99cb7-ee00-44e9-a1b5-3824b85405fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA GenerationConfig. \n",
      "\n",
      " Generation Strategy: \n",
      " \t- do_sample: True \n",
      " \t- num_beams: 1 \n",
      "\n",
      " Model Output Logit manipulation: \n",
      " \t- top-k: 29 \n",
      " \t- top-p: 0.9 \n",
      " \t- min-p: None \n",
      " \t- temperature: 0.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"LLAMA GenerationConfig. \\n\\n \\\n",
    "Generation Strategy: \\n \\\n",
    "\\t- do_sample: {generation_config.do_sample} \\n \\\n",
    "\\t- num_beams: {generation_config.num_beams} \\n\\n \\\n",
    "Model Output Logit manipulation: \\n \\\n",
    "\\t- top-k: {generation_config.top_k} \\n \\\n",
    "\\t- top-p: {generation_config.top_p} \\n \\\n",
    "\\t- min-p: {generation_config.min_p} \\n \\\n",
    "\\t- temperature: {generation_config.temperature}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "560e0549-0958-43cf-b577-4119e1851ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config.top_k = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe2b06dd-ad97-4577-b1ab-d812ff99179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.top_k = 0.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f97a3a3f-988c-48cb-b88e-e0d6bfca9a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6dd4fbe7-e2cf-4f3c-9f1b-d351f06090d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config.use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "30bae637-4a23-487f-b043-13a62b93d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(*generation_config.__dict__.keys(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcbc200-4467-4c52-9381-8e876dbee8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9a934fa7-93d5-4d4a-84b1-f433b418554b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 11 Apr 2025\n",
      "\n",
      "You are an expert in combinatorial optimization over graphs, and I need you to solve a problem<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi ChatGPT. Do you think you can tell me the sum of a and b?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sentences = [\"What is serendipity? Explain in maximum one paragraph.\"]\n",
    "# sentences = [\n",
    "#     {\"role\": \"macaroni\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"What is serendipity? Explain in maximum one paragraph.\"}\n",
    "# ]\n",
    "\n",
    "sentences=[{'role': 'system', 'content': \"You are an expert in combinatorial optimization over graphs, and I need you to solve a problem\"},\n",
    "            {'role': \"user\", \"content\": \"Hi ChatGPT. Do you think you can tell me the sum of a and b?\"}]\n",
    "\n",
    "print(tokenizer.decode(tokenizer.apply_chat_template(sentences, \n",
    "                                                     padding=padding, \n",
    "                                                     return_tensors=return_tensors, \n",
    "                                                     add_generation_prompt=add_generation_prompt)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bcc29f-2109-4806-916c-d57014c93ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "accc328e-aa3e-45f2-9406-fef3238e1441",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [[{\"role\": \"user\", \n",
    "               \"content\": \"Can you extend the given Python code to include more abbreviations and their full names that you might be familiar with?\"\\\n",
    "               \" Also, can you modify the code to print the abbreviations and their full names in alphabetical order?\"}]]*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f191d7d1-3bde-4941-a956-d940096d3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[[{'role': 'user', \n",
    "             'content': \"When did Virgin Australia start operating? Answer in 3 sentences.\\nContext: Virgin Australia, the trading name of \" \\\n",
    "             \"Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin \"\\\n",
    "             \"brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as\"\\\n",
    "             \"a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown\"\\\n",
    "             \"to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"}]]*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "989b46b5-08c0-4f81-9d58-5d9077db85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(sentences, padding=padding, return_tensors='pt', \n",
    "                                       add_generation_prompt=add_generation_prompt,\n",
    "                                       return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7957c6dd-2734-4555-88e4-6e8e761b2193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 155])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4a1a5bc-062e-4cb4-a0e6-7a36a702832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually create attention mask (1s for non-padding tokens)\n",
    "attention_mask = torch.ones_like(inputs).to(device_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d78396b2-64ab-415c-8ca1-3ce0a8bf530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(inputs.to(device_str), attention_mask=attention_mask, generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fcb8121a-0306-432b-bcc7-ff88c223b899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.generation.utils.GenerateDecoderOnlyOutput"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e656ee1-6944-40ec-a541-531de393ccec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'logits', 'hidden_states', 'past_key_values'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c98fb38d-b47a-45d0-bca3-3b07964cc729",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['sequences'] = output.sequences.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1a7c1a23-e3ce-4c4e-945a-82b3000da0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.sequences.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e6bcaf6b-fb01-450a-9f43-555686d07c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output.hidden_states[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b1a525e4-0a1f-435e-b170-d8151d034bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sequences', 'scores', 'logits', 'attentions', 'hidden_states', 'past_key_values'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "303c9896-c510-404e-9fe9-9c222d6faf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output.logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c6e11275-4461-444f-a11d-ac9ea207ba31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128256])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a13bca75-3757-4263-a7d3-e7ee77c64bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len_out_tokens_batch(input_size, output_seq, tokenizer):\n",
    "    assert type(output_seq) == torch.Tensor, \"ERROR: output must a tensor representing the batch of token outputs\"\n",
    "    tmp_output = output_seq[:,input_size:]\n",
    "    # Create a tensor with same size as output filled with the pad_token_id\n",
    "    eos_mask = torch.ones(tmp_output.shape, dtype=tmp_output.dtype).to(tmp_output.device)*tokenizer.pad_token_id\n",
    "    # Return sum of all values of output along batch axis\n",
    "    return torch.sum(tmp_output != eos_mask, dim=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5243a062-f0f5-473f-b736-6ba9f2043760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the outputs (in # of tokens):\n",
      "tensor([67, 66, 77, 58, 77, 67, 60, 83, 76, 85])\n"
     ]
    }
   ],
   "source": [
    "out_lengths = get_len_out_tokens_batch(inputs.shape[1], output['sequences'], tokenizer)\n",
    "print(f\"Length of the outputs (in # of tokens):\\n{out_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7a6cb9a-6c6b-47a5-9e48-3b77db493ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_str = tokenizer.batch_decode(output['sequences'][:, inputs.shape[-1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b13015a-3192-4c17-ad3e-31033a069dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEMENT 0: \n",
      "Virgin Australia, trading as Virgin Australia Airlines Pty Ltd, commenced services on 31 August 2000 under the name Virgin Blue. The airline initially operated with two aircraft on a single route. Following the collapse of Ansett Australia in September 2001, Virgin Blue expanded rapidly and grew to directly serve 32 cities in Australia.\n",
      "SIZE OF ELEM: 67\n",
      "============================================================\n",
      "\n",
      "ELEMENT 1: \n",
      "Virgin Australia, previously known as Virgin Blue, commenced services on 31 August 2000. The airline started with just two aircraft on a single route, marking the beginning of its operations. After the collapse of Ansett Australia in September 2001, Virgin Australia found itself as a major airline in Australia's domestic market.\n",
      "SIZE OF ELEM: 66\n",
      "============================================================\n",
      "\n",
      "ELEMENT 2: \n",
      "Virgin Australia, initially operating as Virgin Blue, commenced services on 31 August 2000 with two aircraft on a single route. After the collapse of Ansett Australia in September 2001, the airline rapidly expanded to become a major player in Australia's domestic market. Today, Virgin Australia directly serves 32 cities in Australia from its hubs in Brisbane, Melbourne, and Sydney.\n",
      "SIZE OF ELEM: 77\n",
      "============================================================\n",
      "\n",
      "ELEMENT 3: \n",
      "Virgin Australia, initially trading as Virgin Blue, commenced services on 31 August 2000. The airline began operating with two aircraft on a single route. After Ansett Australia's collapse in September 2001, Virgin Blue found itself as a major airline in Australia's domestic market.\n",
      "SIZE OF ELEM: 58\n",
      "============================================================\n",
      "\n",
      "ELEMENT 4: \n",
      "Virgin Australia, originally trading as Virgin Blue, commenced services on 31 August 2000 with two aircraft on a single route. After Ansett Australia's collapse in September 2001, Virgin Blue found itself as a major airline in Australia's domestic market. The airline has since grown to directly serve 32 cities in Australia from its hubs in Brisbane, Melbourne, and Sydney.\n",
      "SIZE OF ELEM: 77\n",
      "============================================================\n",
      "\n",
      "ELEMENT 5: \n",
      "Virgin Australia, trading as Virgin Australia Airlines Pty Ltd, commenced services on 31 August 2000 as Virgin Blue. The airline initially operated with just two aircraft on a single route. After Ansett Australia's collapse in September 2001, Virgin Blue rapidly expanded and grew to become a major player in Australia's domestic market.\n",
      "SIZE OF ELEM: 67\n",
      "============================================================\n",
      "\n",
      "ELEMENT 6: \n",
      "Virgin Australia, originally known as Virgin Blue, commenced services on 31 August 2000. The airline started operations with two aircraft on a single route. Following the collapse of Ansett Australia in September 2001, Virgin Blue expanded rapidly to become a major airline in Australia's domestic market.\n",
      "SIZE OF ELEM: 60\n",
      "============================================================\n",
      "\n",
      "ELEMENT 7: \n",
      "Virgin Australia, initially trading as Virgin Blue, commenced services on 31 August 2000 with two aircraft on a single route. The airline was founded after the collapse of Ansett Australia in September 2001, which suddenly elevated it to a major player in Australia's domestic market. Since then, Virgin Australia has expanded to directly serve 32 cities across Australia from hubs in Brisbane, Melbourne, and Sydney.\n",
      "SIZE OF ELEM: 83\n",
      "============================================================\n",
      "\n",
      "ELEMENT 8: \n",
      "Virgin Australia commenced services on 31 August 2000 as Virgin Blue with two aircraft on a single route. The airline started operating under the Virgin Blue name, but later changed its name to Virgin Australia Airlines Pty Ltd in 2007. After the collapse of Ansett Australia in September 2001, Virgin Blue found itself as a major airline in Australia's domestic market.\n",
      "SIZE OF ELEM: 76\n",
      "============================================================\n",
      "\n",
      "ELEMENT 9: \n",
      "Virgin Australia, operating under the trading name Virgin Australia Airlines Pty Ltd, commenced services on 31 August 2000 as Virgin Blue with two aircraft on a single route. The airline initially started with a small fleet but quickly expanded after the collapse of Ansett Australia in September 2001. Over time, Virgin Australia has grown to directly serve 32 cities in Australia from its hubs in Brisbane, Melbourne, and Sydney.\n",
      "SIZE OF ELEM: 85\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,elem in enumerate(output_str):\n",
    "    print(f\"ELEMENT {i}: \\n{elem}\")\n",
    "    print(f\"SIZE OF ELEM: {out_lengths[i]}\")\n",
    "    print(\"============================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e5eb1a-65bc-4d48-969a-a6621f1f50fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85fc943-d382-4779-a9a2-84bbe53ad980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e7c78-3528-4901-ae32-f7bd16a1ab52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30ded7-7503-46c4-8ca5-c41027e652b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92c56d-2d44-4029-962f-6343770f16d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ae4a2-c6d6-427a-bcbe-4ab06db0ba21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce019d2-5623-4097-8f92-dd430166335d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d2549-d76e-4b85-9607-e4d30daadc88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5316153-ae42-4e6f-8d21-b9c076df0fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d1c7a-e8d0-4937-a408-6537fe8f7853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba5bbc-d1c7-45fd-8e4f-4ede8921be67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47b43c-ffdc-4ade-8525-f23678d2f077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e3aa9-f543-43ea-ac65-17f6f5ea8133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70a484c0-4021-4a65-827a-0483ac0e0d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715600ed-e055-4a1f-a639-23ea65ba3734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2805eb7a-dae2-4d57-a69f-8a1fb14509df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_infer(model, generation_config, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids.to('cuda')\n",
    "    output = model.generate(input_ids, \n",
    "                            generation_config=generation_config, \n",
    "                            attention_mask=inputs.attention_mask)\n",
    "    # Return the length of the tokenized input, and the output from \"generate\"\n",
    "    return len(inputs.input_ids[0]), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae348887-4d9f-4a95-827b-2d3280d90377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01663eac-5222-4c91-9744-c0718c585a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_len, output = tokenize_and_infer(model, generation_config, sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e37bb127-65ac-4a12-bdc0-5f297cbc0c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "out_tokens = len(output.logits)\n",
    "print(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "10901d3c-fb3d-4ae6-9bbf-b0f78488d8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,   3923,    374,   1446,    408,    575,    488,     30,  83017,\n",
       "           304,    832,  11914,     13,   8409,    408,    575,    488,    374,\n",
       "           279,  32659,    315,   9455,   2555,  15525,    477,  50189,    994,\n",
       "           499,   3325,   1755,    433,     11,   3629,   1555,   6140,    477,\n",
       "         16907,  13463,     13, 128009], device='cuda:0')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['sequences'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "179fa8ff-435f-4940-b44a-d76b226c0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_text = tokenizer.decode(output['sequences'][0][input_len:], skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7f05c504-7e29-46eb-9088-a67731ab3069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Serendipity is the occurrence of finding something valuable or delightful when you least expect it, often through chance or unexpected circumstances.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "53d2d698-65dd-4d07-988a-447cfa127b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be6d6190-ead6-47f6-a090-44fd418b4e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sequences', 'scores', 'logits', 'attentions', 'hidden_states', 'past_key_values'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ef4848e9-b6e9-4913-bf4a-04543c67d96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fc355709-8049-4138-80e1-c2741ed6d29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128256])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c00d6b-f47d-460b-807c-2e901a4f2375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7a43d368-a41e-4594-b2d2-3a477742777b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.hidden_states[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1c537a52-a8fa-412d-9cfd-1c629b53bee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2048])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.hidden_states[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bcfe70-0fae-44a5-a988-fcc35c32e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output['hidden_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a9593d16-0c58-4787-b8e2-4236b2912626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "What is serendipity? Explain in one sentence. Serendipity is the occurrence of finding something valuable or delightful when you least expect it, often through chance or unexpected circumstances.\n"
     ]
    }
   ],
   "source": [
    "for elem in tokenizer.batch_decode(output['sequences'], skip_special_tokens=True, clean_up_tokenization_spaces=False): \n",
    "    print(f\"###########################\\n{elem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5c4fac6a-ea8b-4851-9253-37a41999802d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output['sequences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ece35452-2aa5-4a51-a4cf-7672d7280a28",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2048) must match the size of tensor b (128256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mall(\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2048) must match the size of tensor b (128256) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "torch.all(output.hidden_states[0][-1][0,-1,:] == output.logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b8466-6b10-44b9-9c75-0b34fe56d9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce4a4c0-4b22-4298-af90-dbdce94df24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7aa96a27-960c-44aa-8736-655e569a2fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1808a96c-d82f-43fc-a7e4-3885e14460cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26b1349-e2b3-4893-b8db-5341455286aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a friendly chatbot who always responds in the style of a pirate<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow many helicopters can a human eat in one sitting?<|eot_id|>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "773da46e-48c4-48bf-8ffc-c833fc968ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4616c191-fd5f-4238-b8af-8151ead160a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1627,  10263,    220,   2366,     19,    271,   2675,    527,\n",
       "            264,  11919,   6369,   6465,    889,   2744,  31680,    304,    279,\n",
       "           1742,    315,    264,  55066, 128009, 128006,    882, 128007,    271,\n",
       "           4438,   1690,  59432,    649,    264,   3823,   8343,    304,    832,\n",
       "          11961,     30, 128009, 128006,  78191, 128007,    271]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a4a40d2-4355-4f97-b617-245eccce3a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How many helicopters can a human eat in one sitting?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_chat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e896e0b-44b0-4ce4-979e-10700f595443",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_name, token=access_tk)\n",
    "generation_config.output_hidden_states = True\n",
    "generation_config.return_dict_in_generate = True\n",
    "generation_config.output_logits = True\n",
    "generation_config.max_new_tokens = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daf27ef4-8e22-436c-8392-9b6ea536cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, generation_config, tokenized_chat, output_states=True):\n",
    "    with torch.no_grad():\n",
    "        t_start = time.time()\n",
    "        output = model.generate(tokenized_chat, generation_config=generation_config)\n",
    "        t_gen_generate = time.time() - t_start\n",
    "    \n",
    "        t_start = time.time()\n",
    "        output_call = model(tokenized_chat, output_hidden_states=True)\n",
    "        t_gen_call = time.time() - t_start\n",
    "        \n",
    "    return output, output_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0333e784-177e-4489-839e-0fec025b8ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1934: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    }
   ],
   "source": [
    "output, output_call = inference(model, generation_config, tokenized_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afbe2a54-d11c-458a-a8fc-56e90ed8d83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sequences', 'scores', 'logits', 'attentions', 'hidden_states', 'past_key_values'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1747d5ee-78f8-4180-be62-80c521b17a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a friendly chatbot who always responds in the style of a pirateuser\n",
      "\n",
      "How many helicopters can a human eat in one sitting?assistant\n",
      "\n",
      "Arrr, ye be askin' a mighty peculiar question, matey!  Eat a helicopter, ye say?  Well, I be thinkin' that be a bit o' a stretch fer even the most adventurous o' eaters.\n",
      "\n",
      "Now, I be knowin' that humans be eatin' all sorts o' things, but a helicopter?  That be a mighty big chunk o' metal and machinery, don't ye think?  I be thinkin' it'd be more like tryin' to swallow a ship, savvy?\n",
      "\n",
      "As fer the actual answer, I be thinkin' it be zero, matey.  Ye can't eat a helicopter, no matter how hungry ye be.  They be made o' metal and wires, not o' edible materials like biscuits or seafood.\n",
      "\n",
      "But if ye be wantin' to know how many helicopters ye could eat if ye could eat 'em, I be thinkin' it be a mighty big number, but still zero, matey!  Ye can't eat a helicopter, no matter how hard ye try!  Now, if ye be wantin' to know about eatin' other things, like seafood or fruit, I be happy to help ye out, matey!\n"
     ]
    }
   ],
   "source": [
    "for elem in tokenizer.batch_decode(output['sequences'], skip_special_tokens=True, clean_up_tokenization_spaces=False): \n",
    "    print(f\"###########################\\n{elem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff88934a-ad3a-419c-b0a5-b6f109b318c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8930f-ff3f-4144-9640-29d33a48edb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c097edf-ec5e-4480-8ca9-09a6db0493a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_tokenize_inference(model, generation_config, input_text, output_states=True):\n",
    "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        t_start = time.time()\n",
    "        output = model.generate(tokenized_chat, generation_config=generation_config)\n",
    "        t_gen_generate = time.time() - t_start\n",
    "    \n",
    "        t_start = time.time()\n",
    "        output_call = model(tokenized_chat, output_hidden_states=True)\n",
    "        t_gen_call = time.time() - t_start\n",
    "        \n",
    "    return output, output_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87659265-d545-47da-aa10-ed83b52511d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_template(question):\n",
    "    return [\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": f\"{question}\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c1aabeb-a813-44e8-ab27-81b7b04a746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"What is serendipity?\", \n",
    "             \"When was Kennedy president?\", \n",
    "             \"Write me a to-do list.\", \n",
    "             \"Alice has 5 apples and Bob has 3. If Bob takes 1 apple from Alice, how many apples do both have at the end?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d07031b-73bf-40ff-b4f3-e88343e3ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [create_chat_template(elem) for elem in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b04b9046-d69d-4c84-97a8-c27a6168a32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1934: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sentence_output = list()\n",
    "reps = 5\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    cache = dict()\n",
    "    for k in range(reps):\n",
    "        output, output_call = chat_tokenize_inference(model, generation_config, sentences[i])\n",
    "        cache[k] = (output, output_call)\n",
    "    sentence_output.append(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f84ff311-9835-4094-8370-ad80435f5a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_determinism(tmp):\n",
    "    # Compare hidden_states between generate() and forward()\n",
    "    if not all([torch.all(tmp[i][0]['hidden_states'][0][0] == tmp[i][1]['hidden_states'][0]).to('cpu') \n",
    "                for i in list(tmp.keys())]):\n",
    "        return False\n",
    "    # Compare logits between generate() and forward()\n",
    "    elif not all([torch.all(tmp[i][0]['logits'][0] == tmp[i][1]['logits'][:,-1,:]).to('cpu') \n",
    "                  for i in list(tmp.keys())]):\n",
    "        return False\n",
    "    # Compare hidden_states across inference runs\n",
    "    elif not all([torch.all(tmp[0][0]['hidden_states'][0][0] == tmp[i][0]['hidden_states'][0][0]).to('cpu') \n",
    "                  for i in range(1, len(list(tmp.keys())))]):\n",
    "        return False\n",
    "    # Compare logits across inference runs\n",
    "    elif not all([torch.all(tmp[0][0]['logits'][0] == tmp[i][0]['logits'][0]).to('cpu') \n",
    "                  for i in range(1, len(list(tmp.keys())))]):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8698cf04-baf9-4b6d-a1cf-3dda24c728bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0 has deterministic hidden states and logits.\n",
      "Sentence 1 has deterministic hidden states and logits.\n",
      "Sentence 2 has deterministic hidden states and logits.\n",
      "Sentence 3 has deterministic hidden states and logits.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentence_output)):\n",
    "    if not check_sentence_determinism(sentence_output[i]):\n",
    "        print(f\"Sentence {i} does not have same values somewhere!\")\n",
    "    else:\n",
    "        print(f\"Sentence {i} has deterministic hidden states and logits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491f23d-1138-4fbc-a276-587729e8c9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e8bb0e-2d99-41b4-bf4f-b90fbcac5c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68a8d28b-68e4-4d68-968e-e36022c75f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6aedbbcb8594afc871fe31691a6e461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45307b150acd49e4a363b882336fd9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67fec61a5aa48c5b6971a7a82e9da21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146618509e2e4c2788cd3f5b0df33a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", token=access_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7310f7e9-1a5f-4346-8544-b960f0d7e6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> [INST] Hello, how are you? [/INST] I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265bc3e-20f3-4d10-9c98-ae1b7b73fd71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d681d1bd-20ba-4ecb-a7b0-4fb0658fa52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
