{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4100218e-3a80-4412-9402-5560a03a2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "import accelerate\n",
    "import torch\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import inspect\n",
    "\n",
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "264759e1-f672-497a-b01a-cf8251a43d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_tk = os.getenv(\"HF_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c453e31f-5028-42ec-8bda-11ce9a4e3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f686d46-5027-4643-a88a-b948569e3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_metadata(model_name : str, device_str: str = \"auto\"):\n",
    "    print(f\"###########\\nPROCESSING MODEL {model_name}\\n\")\n",
    "    padding_side='left'\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, token=access_tk, device_map=device_str, \n",
    "                                                 torch_dtype=torch.float16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_tk, device_map=device_str, \n",
    "                                              torch_dtype=torch.float16, padding_side=padding_side)\n",
    "    generation_config = GenerationConfig.from_pretrained(model_name, token=access_tk)\n",
    "\n",
    "    ret_dict = {'generation_strategy': {'do_sample': generation_config.do_sample, \n",
    "                                        'num_beams': generation_config.num_beams},\n",
    "                'outpit_logit_config': {'top_k': generation_config.top_k, \n",
    "                                        'top_p': generation_config.top_p, \n",
    "                                        'min-p': generation_config.min_p,\n",
    "                                        'temperature': generation_config.temperature},\n",
    "                'input_context_length': model.config.max_position_embeddings,\n",
    "               }\n",
    "    del model\n",
    "    del tokenizer\n",
    "    \n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2fd26c4-77d9-4dfc-aa9f-86c4f51c89fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "PROCESSING MODEL meta-llama/Llama-3.2-1B-Instruct\n",
      "\n",
      "###########\n",
      "PROCESSING MODEL meta-llama/Llama-3.2-3B-Instruct\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e0a44d4ac24708b65d569dc31f4519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "PROCESSING MODEL google/gemma-2-2b-it\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a641a5b2ce4f309e5de64e707286b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "PROCESSING MODEL mistralai/Mistral-7B-Instruct-v0.1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8ab98d736b429784529279c1f7ff6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "PROCESSING MODEL meta-llama/Llama-3.1-8B-Instruct\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9061cc1e794a65b6b1e1b3edb9f180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models_raw_metainfo = {name: get_model_metadata(name) for name in model_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8af41c6-29d7-4817-b79f-9713e9b5dcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta-llama/Llama-3.2-1B-Instruct': {'generation_strategy': {'do_sample': True,\n",
       "   'num_beams': 1},\n",
       "  'outpit_logit_config': {'top_k': 50,\n",
       "   'top_p': 0.9,\n",
       "   'min-p': None,\n",
       "   'temperature': 0.6},\n",
       "  'input_context_length': 131072},\n",
       " 'meta-llama/Llama-3.2-3B-Instruct': {'generation_strategy': {'do_sample': True,\n",
       "   'num_beams': 1},\n",
       "  'outpit_logit_config': {'top_k': 50,\n",
       "   'top_p': 0.9,\n",
       "   'min-p': None,\n",
       "   'temperature': 0.6},\n",
       "  'input_context_length': 131072},\n",
       " 'google/gemma-2-2b-it': {'generation_strategy': {'do_sample': False,\n",
       "   'num_beams': 1},\n",
       "  'outpit_logit_config': {'top_k': 50,\n",
       "   'top_p': 1.0,\n",
       "   'min-p': None,\n",
       "   'temperature': 1.0},\n",
       "  'input_context_length': 8192},\n",
       " 'mistralai/Mistral-7B-Instruct-v0.1': {'generation_strategy': {'do_sample': False,\n",
       "   'num_beams': 1},\n",
       "  'outpit_logit_config': {'top_k': 50,\n",
       "   'top_p': 1.0,\n",
       "   'min-p': None,\n",
       "   'temperature': 1.0},\n",
       "  'input_context_length': 32768},\n",
       " 'meta-llama/Llama-3.1-8B-Instruct': {'generation_strategy': {'do_sample': True,\n",
       "   'num_beams': 1},\n",
       "  'outpit_logit_config': {'top_k': 50,\n",
       "   'top_p': 0.9,\n",
       "   'min-p': None,\n",
       "   'temperature': 0.6},\n",
       "  'input_context_length': 131072}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_raw_metainfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48b56a4c-29a4-4249-8e79-620079399180",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_file = \"../data/models_raw_metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca912a1-abb0-4346-b6ab-dcfa379b1fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dst_file, 'w') as f:\n",
    "    json.dump(models_raw_metainfo, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e61bc-a5b0-4bda-b888-76b70218d68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ff06d2d-b776-4fc9-9b28-6215cf782dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37ed96a5-e90f-491b-a9ce-f7fe5677cf71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3402e61-8ddc-4294-8348-37311b7a85b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15110963200, 25425608704)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.mem_get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e54948eb-4c8d-4803-8901-b5057f6d6dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10314645504"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25d9bde5-fef9-4c52-88e8-90204b797aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7218397184"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59a3618f-17b8-4df5-861b-af5ce8fba594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2466c87d-3b00-4900-8861-0656f9f4a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([torch.cuda.memory_reserved(i) - torch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50df135-e918-45ae-8398-ea48808dbb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd472c-81d4-4abd-8d4d-5b3b161f912d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5bc79-a677-48f0-8239-aed16ebdff56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec3284-bc2f-4789-a3b7-2c7b227f56ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f0fe8-7ed3-44f8-9391-cc5555166ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90f0b4-0482-4ef1-91ba-53955d389da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf871c-ec8a-4487-8212-c1c33803d9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd534b75-0d25-43a9-9562-3bea135a3645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40299e7e-cd3a-40e4-ba3c-33eecec1a551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
